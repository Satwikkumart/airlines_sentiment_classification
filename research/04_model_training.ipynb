{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746e8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72d3683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd87ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfad8698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwik/Downloads/MLproj/airlines_sentiment_classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f975f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    datasets_dir: Path\n",
    "    base_model_path: Path\n",
    "    output_dir: Path\n",
    "    model_save_path: Path\n",
    "    num_train_epochs: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    warmup_steps: int\n",
    "    weight_decay: float\n",
    "    max_steps: int\n",
    "    save_steps: int\n",
    "    logging_steps: int\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ededbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba454487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a283523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usr/bin/spam'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join('usr', 'bin', 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f73a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airlinesSentiment.constants import *\n",
    "from airlinesSentiment.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67788ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath=CONFIG_FILE_PATH,\n",
    "            params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.feature_engineering.datasets_dir, \"datasets\")\n",
    "        base_model = self.config.prepare_model\n",
    "        base_model = os.path.join(self.config.prepare_model.base_model_path, \"prepare_model\")\n",
    "\n",
    "\n",
    "        #create directories if they don't exist\n",
    "        create_directories([\n",
    "            Path(training.root_dir),\n",
    "            Path(training.model_save_path)\n",
    "\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(training_data),  #Update this to point to feature_engineering\n",
    "            base_model_path=Path(base_model),\n",
    "            output_dir=Path(training.root_dir),\n",
    "            model_save_path=Path(training.model_save_path),\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            weight_decay=params.weight_decay,\n",
    "            max_steps=params.max_steps,\n",
    "            save_steps=params.save_steps,\n",
    "            logging_steps=params.logging_steps\n",
    "        )\n",
    "\n",
    "        return training_config\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e05d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9d82bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from airlinesSentiment import logger\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config:TrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\" Loads the train, validation and test datasets from the specified directory\n",
    "        Returns:\n",
    "            train_datasets, val_datasets, test_datasets: Loaded datasets\"\"\"\n",
    "        datasets = Path(\"artifacts/feature_engineering/datasets\")\n",
    "        print(self.config.datasets_dir)\n",
    "        print(datasets)\n",
    "\n",
    "        train_datasets = torch.load(datasets / \"train_dataset.pt\", weights_only=False)\n",
    "        val_datasets = torch.load(datasets / \"val_dataset.pt\", weights_only=False)\n",
    "        test_datasets = torch.load(datasets / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets}\")\n",
    "        return train_datasets, val_datasets, test_datasets\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        #load datasets \n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        #initialize the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "        # model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "        #set up training aruguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.outputs_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            # load_best_model_at_end=True\n",
    "\n",
    "        )\n",
    "\n",
    "        # Initialize the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        self.save_model(trainer)\n",
    "\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\" Saves the trainer model and tokenizer to the specified directoty\n",
    "        Args:\n",
    "            trainer(Trainer): The Trainer object containing the trained model\"\"\"\n",
    "        \n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "        # # Save the tokenizer\n",
    "        # tokenizer = trainer.tokenizer\n",
    "        # if tokenizer is not None:\n",
    "        #     tokenizer.save_pretrained(save_path)\n",
    "        #     logger.info(f\"Tokenizer saved to {save_path}\")\n",
    "\n",
    "        # else:\n",
    "        #     logger.warning(\"Tokenizer not found  in the trainer object. Only the model was saved\")\n",
    "\n",
    "        # Save the processing class(e.g Tokenizer)\n",
    "        processing_class = getattr(trainer, \"processing_class\", None)\n",
    "        if processing_class is not None:\n",
    "            processing_class.save_pretrained(save_path)\n",
    "            logger.info(f\"Processing class (e.g tokenizer) saved to path {save_path}\")\n",
    "\n",
    "        else:\n",
    "            logger.warning(\"Processing class not found. Only the model was saved \")\n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test dataset\"\"\"\n",
    "\n",
    "        # Load, the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.outputs_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Set up taining arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.outputs_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.outputs_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation \n",
    "\n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(\"Evaluation Results: \", results)\n",
    "        logger.info(f\"     - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"     - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"     - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"     - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"     - Epoch: {results.get('epoch', 'N/A')}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4775477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import torch \n",
    "from airlinesSentiment import logger\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\" Initialize the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "        config (TrainingConfig): Configuration for Model Training\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\" \n",
    "        Loads the train, validation and test datasets from the specified directory\"\"\"\n",
    "\n",
    "\n",
    "        #Explicitly set the corrupt path\n",
    "        datasets = Path(\"artifacts/feature_engineering/datasets\")\n",
    "\n",
    "        #Debug: print the datasets directory\n",
    "        print(\"Datasets Directory:\", datasets)\n",
    "\n",
    "        #Load Datasets\n",
    "        train_dataset = torch.load(datasets / \"train_dataset.pt\", weights_only=False)\n",
    "        val_dataset = torch.load(datasets / \"val_dataset.pt\", weights_only=False)\n",
    "        test_dataset = torch.load(datasets / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\" Trains the model using laoded dataset\"\"\"\n",
    "\n",
    "        # Load datasets \n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Load the base model and tokenizer from the artifacts/ prepare base model folder \n",
    "        #base_model_path = Path(self.config.base_model_path)\n",
    "        base_model_path = Path(\"artifacts/prepare_model\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(base_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "        # set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "\n",
    "        )\n",
    "\n",
    "        #Initialize the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        #start training\n",
    "        trainer.train()\n",
    "\n",
    "        #save the trained model\n",
    "\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    \n",
    "    def save_model(self, trainer):\n",
    "        \"\"\" \n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "        Args:\n",
    "            trainer(Trainer): The trainer object containing the trained model\"\"\"\n",
    "        \n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # This is the single, correct line to save both the model and tokenizer\n",
    "        trainer.save_model(save_path)\n",
    "    \n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "        # Save the model\n",
    "        tokenizer = trainer.tokenizer\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            logger.info(f\"Tokenizer saved to {save_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Tokenizer not found in the trainer object. Only the model was saved\")\n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\" \n",
    "        Evaluate the model on the test dataset.\"\"\"\n",
    "\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        #load the trained model from the model_save_path\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "        #Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        #Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args\n",
    "        )\n",
    "\n",
    "        #Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results: \")\n",
    "        logger.info(f\"     - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"     - Runtime: {results['eval_runtime']:.2f} Seconds\")\n",
    "        logger.info(f\"     - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"     - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"     - Epoch: {results.get('epoch', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b36afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 02:35:11,021: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-08-11 02:35:11,027: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-11 02:35:11,030: INFO: common: created directory at: artifacts]\n",
      "[2025-08-11 02:35:11,031: INFO: common: created directory at: artifacts/training]\n",
      "[2025-08-11 02:35:11,032: INFO: common: created directory at: artifacts/training/trained_model]\n",
      "Datasets Directory: artifacts/feature_engineering/datasets\n",
      "[2025-08-11 02:35:11,482: INFO: 422766164: Datasets loaded from artifacts/feature_engineering/datasets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305715272939425a9a9313d2841bcf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c9c27917374259aa84ce599e287ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.030842900276184, 'eval_runtime': 19.268, 'eval_samples_per_second': 113.971, 'eval_steps_per_second': 56.986, 'epoch': 0.0}\n",
      "{'train_runtime': 65.3165, 'train_samples_per_second': 0.306, 'train_steps_per_second': 0.153, 'train_loss': 1.0909061431884766, 'epoch': 0.0}\n",
      "[2025-08-11 02:36:22,103: INFO: 422766164: Model and tokenizer saved to artifacts/training/trained_model]\n",
      "[2025-08-11 02:36:22,118: INFO: 422766164: Tokenizer saved to artifacts/training/trained_model]\n",
      "Datasets Directory: artifacts/feature_engineering/datasets\n",
      "[2025-08-11 02:36:22,753: INFO: 422766164: Datasets loaded from artifacts/feature_engineering/datasets]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72284c9f9add417d8e70b4e6d87c0613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-11 02:36:40,369: INFO: 422766164: Evaluation Results: ]\n",
      "[2025-08-11 02:36:40,370: INFO: 422766164:      - Loss: 1.0390]\n",
      "[2025-08-11 02:36:40,372: INFO: 422766164:      - Runtime: 16.50 Seconds]\n",
      "[2025-08-11 02:36:40,372: INFO: 422766164:      - Samples per Second: 133.09]\n",
      "[2025-08-11 02:36:40,374: INFO: 422766164:      - Steps per Second: 66.55]\n",
      "[2025-08-11 02:36:40,375: INFO: 422766164:      - Epoch: N/A]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "accelerator_state_kwargs = {\"enabled\": True, \"use_configured_state\": False}\n",
    "\n",
    "\n",
    "# Initialize Partialstate\n",
    "\n",
    "# from  airlinesSentiment.components.feature_engineering import SentimentDataset\n",
    "if __name__ == \"__main__\":\n",
    "    #Initialize the configuration manager\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    #get the model trainig config \n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    #Initialize the ModelTraining\n",
    "    model_training = ModelTraining(config=training_config)\n",
    "\n",
    "    #Train the model\n",
    "    model_training.train()\n",
    "\n",
    "    #Evaluate the model\n",
    "    model_training.evaluate()\n",
    "\n",
    "    partial_state = PartialState()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7e1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ea24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_envv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
