{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b96e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abe407d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e4cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9bd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7179f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwik/Downloads/MLproj/airlines_sentiment_classification'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d3557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    training_data_path : Path\n",
    "    training_data_file : Path\n",
    "    training_cleansed_data : Path\n",
    "    datasets_dir : Path\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f05ed6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"/Users/satwik/Downloads/MLproj/src/airlines_sentiment_classification/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/params.yaml\")\n",
    "# Path(\"src/airlinesSentiment/config/config.yaml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3decb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for file at: /Users/satwik/Downloads/MLproj/airlines_sentiment_classification/config/config.yaml\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# print(f\"Checking for file at: {CONFIG_FILE_PATH}\")\n",
    "# print(f\"File exists: {os.path.exists(CONFIG_FILE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "971dd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = Path(\"/Users/satwik/Downloads/MLproj/airlines_sentiment_classification/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a333a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.airlinesSentiment.constants import *\n",
    "from src.airlinesSentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faee9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.feature_engineering\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            training_data_path=config.training_data_path,\n",
    "            training_data_file=config.training_data_file,\n",
    "            training_cleansed_data=config.training_cleansed_data,\n",
    "            datasets_dir=config.datasets_dir\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "707f396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:33:37,804: INFO: 3687390697: Text tokenization completed]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from airlinesSentiment import logger\n",
    "from airlinesSentiment.utils.common import get_size\n",
    "from torch.utils.data import Dataset\n",
    "import gdown\n",
    "import spacy\n",
    "import torchgen\n",
    "import logging\n",
    "import torch\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "import string\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for sentiment analysis.\n",
    "\n",
    "        Args:\n",
    "            encodings (dict): Tokenized encodings (e.g., input_ids, attention_mask).\n",
    "            labels (list): List of labels corresponding to the encodings.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "        \n",
    "class DataPreprocessing: \n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.mapping_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "        self.data = self.load_data()\n",
    "        self.output_file_path = Path(self.config.training_cleansed_data) / 'cleaned_tweets.csv'\n",
    "        self.datasets_dir = Path(self.config.datasets_dir)\n",
    "\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        if not Path(self.config.training_data_file).exists():\n",
    "            raise FileNotFoundError(f'file not found: {self.config.training_data_file}')\n",
    "        \n",
    "        #load the dataset\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "        logger.info(f\"Dataset loaded from {self.config.training_data_file}\")\n",
    "        return data\n",
    "    \n",
    "    def text_process(self) -> None:\n",
    "\n",
    "        self.data['cleaned_text'] = self.data['text'].apply(self._process_single_text)\n",
    "        logger.info(\"Text processing completed\")\n",
    "\n",
    "\n",
    "\n",
    "    def _process_single_text(self, text: str) -> str:\n",
    "\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def mapping_labels_func(self) -> None:\n",
    "\n",
    "        self.data['labels'] = self.data['airline_sentiment'].map(self.mapping_labels)\n",
    "        logger.info(\"Labels mapped to numerical values\")\n",
    "\n",
    "    def tokenize_text(self) -> None:\n",
    "        self.data['tokenized'] = self.data['cleaned_text'].apply(\n",
    "            lambda x: self.tokenizer(x, padding = 'max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        )\n",
    "    logger.info(\"Text tokenization completed\")\n",
    "\n",
    "    def save_data(self) -> None:\n",
    "        self.data.to_csv(self.output_file_path, index=False)\n",
    "        logger.info(f\"Preprocessed data saved to {self.output_file_path}\")\n",
    "\n",
    "    def train_val_test_split(self, test_size: float = 0.3, val_size: float = 0.5, random_state: int = 42) -> dict:\n",
    "        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "            self.data['cleaned_text'].to_list(), self.data['labels'].to_list(), test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "        logger.info(\"Data split into test, train and validation sets \")\n",
    "        return {\n",
    "            'train': {'texts': train_texts, 'labels': train_labels},\n",
    "            'val': {'texts': val_texts, 'labels': val_labels },\n",
    "            'test': {'texts': test_texts, 'labels': test_labels}\n",
    "\n",
    "        }\n",
    "\n",
    "    def covert_to_tokenized_datasets(self, splits: dict) -> dict:\n",
    "\n",
    "        #tokenize the texts\n",
    "        train_encodings = self.tokenizer(splits['train']['texts'], truncation=True, padding=True, max_length=128)\n",
    "        val_enocdings = self.tokenizer(splits['val']['texts'], truncation=True, padding=True, max_length=128)\n",
    "        test_encodings = self.tokenizer(splits['test']['texts'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "        #create pytorch datasets\n",
    "        train_dataset = SentimentDataset(train_encodings, splits['train']['labels'])\n",
    "        val_dataset = SentimentDataset(val_enocdings, splits['val']['labels'])\n",
    "        test_dataset = SentimentDataset(test_encodings, splits['val']['labels'])\n",
    "\n",
    "        logger.info(\"Pytorch datasets created succesffuly.\")\n",
    "        return {\n",
    "            'train' : train_dataset,\n",
    "            'val' : val_dataset,\n",
    "            'test' : test_dataset\n",
    "        }\n",
    "    def save_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        #create the directory if the directory doesn't exists\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #save the datasets\n",
    "        torch.save(train_dataset, datasets_dir / \"train_dataset.pt\")\n",
    "        torch.save(val_dataset, datasets_dir / \"val_dataset.pt\")\n",
    "        torch.save(test_dataset, datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "        logger.info(f\"Datasets saved to {datasets_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f821b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:33:41,943: INFO: common: yaml file: /Users/satwik/Downloads/MLproj/airlines_sentiment_classification/src/airlinesSentiment/config/config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:33:41,949: INFO: common: yaml file: /Users/satwik/Downloads/MLproj/airlines_sentiment_classification/src/airlinesSentiment/params.yaml loaded successfully]\n",
      "[2025-08-08 15:33:41,954: INFO: common: created directory at: artifacts]\n",
      "[2025-08-08 15:33:41,958: INFO: common: created directory at: artifacts/feature_engineering]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/llm_envv/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-08 15:33:43,086: INFO: 3687390697: Dataset loaded from artifacts/feature_engineering/Tweets.csv]\n",
      "[2025-08-08 15:34:58,038: INFO: 3687390697: Text processing completed]\n",
      "[2025-08-08 15:34:58,045: INFO: 3687390697: Labels mapped to numerical values]\n",
      "[2025-08-08 15:35:05,575: INFO: 3687390697: Preprocessed data saved to artifacts/feature_engineering/cleaned_tweets.csv]\n",
      "[2025-08-08 15:35:05,599: INFO: 3687390697: Data split into test, train and validation sets ]\n",
      "[2025-08-08 15:35:07,559: INFO: 3687390697: Pytorch datasets created succesffuly.]\n",
      "[2025-08-08 15:35:07,561: INFO: 3882184486: PyTorch data created successfully]\n",
      "[2025-08-08 15:35:08,104: INFO: 3687390697: Datasets saved to artifacts/feature_engineering/datasets]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_data_pre_config = config.get_data_preprocessing_config()\n",
    "    data_pre_process = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "    #prepare the data\n",
    "    data_pre_process.text_process()\n",
    "    data_pre_process.mapping_labels_func()\n",
    "    data_pre_process.tokenize_text()\n",
    "    \n",
    "\n",
    "    #save the preprocessed data\n",
    "    data_pre_process.save_data()\n",
    "\n",
    "    #splits the dataset into train, validation and test sets\n",
    "    splits = data_pre_process.train_val_test_split()\n",
    "\n",
    "    #converts split into PyTorch dataset\n",
    "    tokenized_datasets = data_pre_process.covert_to_tokenized_datasets(splits)\n",
    "\n",
    "    # Access the PyTorch datasets \n",
    "    train_dataset = tokenized_datasets['train']\n",
    "    val_dataset = tokenized_datasets['val']\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "    logger.info(\"PyTorch data created successfully\")\n",
    "\n",
    "    datasets = data_pre_process.save_datasets(train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data preprocessing {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d4595c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/satwik/Downloads/MLproj/airlines_sentiment_classification'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b186f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc2870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b4af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade20874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e1254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bcbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72213b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_envv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
